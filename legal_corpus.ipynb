{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import required packages\n",
    "import nltk\n",
    "import pandas as pd\n",
    "import csv\n",
    "import re\n",
    "import numpy as np\n",
    "import os\n",
    "import gensim\n",
    "import glob\n",
    "import spacy\n",
    "import random\n",
    "import matplotlib.pyplot\n",
    "from nltk.tokenize import sent_tokenize\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk import ngrams\n",
    "from gensim import corpora, models\n",
    "import pyLDAvis\n",
    "import pyLDAvis.gensim\n",
    "import pyLDAvis.gensim_models as gensimvis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "217\n",
      "452\n",
      "644\n",
      "855\n",
      "971\n"
     ]
    }
   ],
   "source": [
    "#Generate file names using glob\n",
    "_2018filenames = [I for I in glob.glob('data/2018/*.txt')]\n",
    "_2019filenames = [I for I in glob.glob('data/2019/*.txt')]\n",
    "_2020filenames = [I for I in glob.glob('data/2020/*.txt')]\n",
    "_2021filenames = [I for I in glob.glob('data/2021/*.txt')]\n",
    "_2022filenames = [I for I in glob.glob('data/2022/*.txt')]\n",
    "\n",
    "#TO REMOVE: TEST CASE FOR BIGRAMS\n",
    "# test_data = open(r\"./data/TEST1.txt\", \"r\").read()\n",
    "# list_test = list()\n",
    "# list_test.append(test_data)\n",
    "\n",
    "#Save them as one massive list\n",
    "list_cases = list()\n",
    "\n",
    "\n",
    "for file in _2018filenames:\n",
    "    case = open(r\"\" + file + \"\", \"r\")\n",
    "    case = case.read()\n",
    "    list_cases.append(case)\n",
    "print(len(list_cases))\n",
    "\n",
    "for file in _2019filenames:\n",
    "    case = open(r\"\" + file + \"\", \"r\")\n",
    "    case = case.read()\n",
    "    list_cases.append(case)\n",
    "print(len(list_cases))\n",
    "\n",
    "for file in _2020filenames:\n",
    "    case = open(r\"\" + file + \"\", \"r\")\n",
    "    case = case.read()\n",
    "    list_cases.append(case)\n",
    "print(len(list_cases))\n",
    "\n",
    "for file in _2021filenames:\n",
    "    case = open(r\"\" + file + \"\", \"r\")\n",
    "    case = case.read()\n",
    "    list_cases.append(case)\n",
    "\n",
    "print(len(list_cases))\n",
    "for file in _2022filenames:\n",
    "    case = open(r\"\" + file + \"\", \"r\")\n",
    "    case = case.read()\n",
    "    list_cases.append(case)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create helper functions for pre-processing\n",
    "\n",
    "#Default stopwords list\n",
    "from nltk.corpus import stopwords\n",
    "stopwords_def = stopwords.words('english')\n",
    "\n",
    "#Keep only real words\n",
    "from nltk.corpus import words\n",
    "from nltk.corpus import wordnet\n",
    "\n",
    "#Define words as anything distinct in these two NLTK sets\n",
    "real_word_set = set(words.words() + list(wordnet.words()))\n",
    "    \n",
    "#Depluralize all nouns\n",
    "import pattern\n",
    "from pattern.en import singularize\n",
    "\n",
    "#Find words that end with 's' that need to be handled differently than plurals\n",
    "exceptions = pd.read_csv('./preprocessing/singularized_exceptions.csv')\n",
    "exceptions_list = exceptions.to_dict('records')\n",
    "exceptions_list = exceptions_list[0]\n",
    "\n",
    "#Set a function to process these words differently\n",
    "def singularize_esp(word):\n",
    "    if word in exceptions_list:\n",
    "        return exceptions_list.get(word)\n",
    "    return singularize(word)\n",
    "    \n",
    "#Want to keep certain words capitalized\n",
    "proper_nouns = pd.read_csv('./preprocessing/proper_nouns.csv', header = None)\n",
    "proper_nouns = proper_nouns.values.tolist()\n",
    "proper_nouns = proper_nouns[0]\n",
    "\n",
    "def recapitalize(word):\n",
    "    if word in proper_nouns:\n",
    "        return word\n",
    "    if word.lower() in real_word_set:\n",
    "        return word.lower()   \n",
    "    else:\n",
    "        return word\n",
    "    \n",
    "#n-gram helper function\n",
    "def replace_ngram(x):\n",
    "    for gram in bigrams:\n",
    "        x = x.replace(gram, '_'.join(gram.split()))\n",
    "    for gram in trigrams:\n",
    "        x = x.replace(gram, '_'.join(gram.split()))\n",
    "    return x\n",
    "\n",
    "#Lemmatize based on part of speech (pos tagging)\n",
    "\n",
    "def pos_lemmatize(word, tag):\n",
    "    if tag.startswith('J'):\n",
    "        return lemmatizer.lemmatize(word, pos='a')  # Adjective\n",
    "    elif tag.startswith('V'):\n",
    "        return lemmatizer.lemmatize(word, pos='v')  # Verb\n",
    "    elif tag.startswith('N'):\n",
    "        return lemmatizer.lemmatize(word, pos='n')  # Noun\n",
    "    elif tag.startswith('R'):\n",
    "        return lemmatizer.lemmatize(word, pos='r')  # Adverb\n",
    "    else:\n",
    "        return lemmatizer.lemmatize(word)  # Default to noun\n",
    "\n",
    "#Get rid of proper nouns\n",
    "name_remove = spacy.load(\"en_core_web_sm\")\n",
    "def remove_proper_nouns(text):\n",
    "    doc = name_remove(text)\n",
    "    filtered_text = \" \".join([token.text for token in doc if token.pos_ != \"PROPN\"])\n",
    "    return filtered_text\n",
    "\n",
    "#Filter to keep only nouns\n",
    "def nouns(word):\n",
    "    pos_word = nltk.pos_tag(word)\n",
    "    filtered = [word[0] for word in pos_word if word[1] in ['NN', 'NNP', 'NNS', 'NNPS']]\n",
    "    return filtered "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from nltk.probability import FreqDist\n",
    "\n",
    "legal_corpus = pd.DataFrame()\n",
    "\n",
    "for i in range(10):\n",
    "    selected_samples = random.sample(list_cases, 25)\n",
    "    lemmatized_documents = []\n",
    "    for text in selected_samples: \n",
    "        draft_case = doc\n",
    "        #Use regex to do a preliminary cleaning of the content\n",
    "        ### Remove punctuation and numbers\n",
    "        draft_case = re.sub(\"[0-9]\", \"\", draft_case) #Removes numbers\n",
    "        draft_case = re.sub(r\"\\/\", \" \", draft_case) #Replaces slashes with spaces\n",
    "        draft_case = re.sub(r\"\\'\", \" \", draft_case) #Replaces apostrophes with spaces\n",
    "        draft_case = re.sub(r\"\\-\", \" \", draft_case) #Replaces en-dash with spaces\n",
    "        draft_case = re.sub(r\"\\–\", \" \", draft_case) #Replaces em-dash with spaces\n",
    "        draft_case = re.sub(r\"[.,?!()•$;:@§%&~\\[\\]\\\"]\", \"\", draft_case) #Removes extraneous punctuation\n",
    "        draft_case = re.sub(r\"^$n','\\n\", \"\", draft_case, re.MULTILINE) #Removes empty lines\n",
    "        draft_case = re.sub(r\"\\b\\w{1,2}\\b\", '', draft_case) #Removes all one and two-character words (none have meaning)\n",
    "        draft_case = re.sub(r\"\\n\",\"\", draft_case) #Removes extraneous line breaks\n",
    "\n",
    "\n",
    "        #Remove extraneous information about lawyers and case detail that doesn't impact the content of the case\n",
    "        starting_phrase = \"PRELIMINARY STATEMENT\" #Use this as the starter for when the actual lawsuit begins\n",
    "        remove_before = r'^.*?{}'.format(re.escape(starting_phrase))\n",
    "        draft_case = re.sub(remove_before, \"\", draft_case, flags=re.DOTALL)\n",
    "\n",
    "        #Remove \"FIRST AMENDED COMPLAINT\", which is at the bottom of every page\n",
    "        draft_case = re.sub(r\"FIRST AMENDED COMPLAINT\", \"\", draft_case)\n",
    "\n",
    "        #These steps are done last since other words' removal will impact them\n",
    "        draft_case = re.sub(r\"\\s+\", \" \", draft_case) #Removes multiple whitespaces\n",
    "        draft_case = re.sub(r\"([a-z])([A-Z])\", r\"\\1 \\2\", draft_case) #Separates words that were joined together by double spaces\n",
    "\n",
    "        #Word tokenization\n",
    "        draft_case = word_tokenize(draft_case)\n",
    "        print(draft_case[0:100])\n",
    "\n",
    "\n",
    "        #Singularize\n",
    "        draft_case = [singularize_esp(word) for word in draft_case]\n",
    "\n",
    "        #Recapitalize proper nouns only\n",
    "        draft_case = [recapitalize(word) for word in draft_case]\n",
    "\n",
    "        #Remove stopwords\n",
    "        #Remove all non-real words (done twice since wordnet \"real words\" set includes names)\n",
    "        draft_case = [word for word in draft_case if ((word not in stopwords_def) and (word in real_word_set or word in proper_nouns))]\n",
    "        draft_case = [remove_proper_nouns(word) for word in draft_case]\n",
    "\n",
    "        #After removing non-real words, implement n-grams\n",
    "\n",
    "        ##Return draft_case into a string\n",
    "        draft_case = ' '.join(draft_case)\n",
    "        ##Create n-grams using pointwise mutual information\n",
    "\n",
    "        ###Create bigrams and use PMI to score them\n",
    "        bigram_measures = nltk.collocations.BigramAssocMeasures()\n",
    "        finder = nltk.collocations.BigramCollocationFinder.from_words(str.split(draft_case))\n",
    "        ####Apply a frequency filter\n",
    "        finder.apply_freq_filter(3)\n",
    "        bigram_scores = finder.score_ngrams(bigram_measures.pmi)\n",
    "\n",
    "        ###Do the same for trigrams\n",
    "        trigram_measures = nltk.collocations.TrigramAssocMeasures()\n",
    "        finder = nltk.collocations.TrigramCollocationFinder.from_words(str.split(draft_case))\n",
    "        ####Frequency filter\n",
    "        finder.apply_freq_filter(3)\n",
    "        trigram_scores = finder.score_ngrams(trigram_measures.pmi)\n",
    "\n",
    "        #Store the scores as dataframes\n",
    "        bigram_pmi = pd.DataFrame(bigram_scores)\n",
    "        bigram_pmi.columns = ['bigram', 'pmi']\n",
    "        bigram_pmi.sort_values(by='pmi', axis = 0, ascending = False, inplace = True)\n",
    "        #Save only the bigrams with PMI greater than 5 (arbitrary threshold)\n",
    "        bigram_pmi = bigram_pmi[bigram_pmi.apply(lambda bigram: bigram.pmi > 5, axis = 1)][:500]\n",
    "\n",
    "        trigram_pmi = pd.DataFrame(trigram_scores)\n",
    "        trigram_pmi.columns = ['trigram', 'pmi']\n",
    "        trigram_pmi.sort_values(by='pmi', axis = 0, ascending = False, inplace = True)\n",
    "        #Save only the trigrams with PMI greater than 5 (arbitrary threshold)\n",
    "        trigram_pmi = trigram_pmi[trigram_pmi.apply(lambda trigram: trigram.pmi > 5, axis = 1)][:500]\n",
    "\n",
    "        #Keep only the values\n",
    "        bigrams = [' '.join(x) for x in bigram_pmi.bigram.values]\n",
    "        trigrams = [' '.join(x) for x in trigram_pmi.trigram.values]\n",
    "\n",
    "        # Concatenate n-grams\n",
    "        draft_case = replace_ngram(draft_case)\n",
    "\n",
    "        #Lemmatization\n",
    "        from nltk.stem import WordNetLemmatizer\n",
    "        lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "        #Re-tokenize\n",
    "        draft_case = word_tokenize(draft_case)\n",
    "\n",
    "        #Tag by part of speech\n",
    "        from nltk import pos_tag\n",
    "        tagged_word_content = pos_tag(draft_case)\n",
    "\n",
    "        #Need to lemmatize using part of speech to ensure accuracy\n",
    "        # Lemmatize using part of speech\n",
    "        lemmatized_output = [pos_lemmatize(word, tag) for word, tag in tagged_word_content]\n",
    "        lemmatized_output = nouns(lemmatized_output)\n",
    "        \n",
    "        lemmatized_documents.append(lemmatized_output)\n",
    "\n",
    "    flattened_corpus = [word for doc in selected_samples for word in doc]\n",
    "    # Create a frequency distribution\n",
    "    freq_dist = FreqDist(flattened_corpus)\n",
    "    # Get the most common words\n",
    "    most_common_words = freq_dist.most_common(100)\n",
    "    column_name_words = f\"Sample_{i+1}_Words\"\n",
    "    column_name_count = f\"Sample_{i+1}_Count\"\n",
    "    legal_corpus[column_name_words] = [word for word, _ in most_common_words]\n",
    "    legal_corpus[column_name_count] = [freq for _, freq in most_common_words]\n",
    "\n",
    "legal_corpus.to_csv('preprocessing/legal_corpus.csv')\n",
    "        "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
